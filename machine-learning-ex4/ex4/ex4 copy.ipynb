{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Neural Network - Backpropagation\n\n## Terminology\n\n$ s_l $ : no of nodes in layer $ l $  \n$ a^{(l)} $ : activation nodes for layer $ l $ , has dimension $ [m \\times (s_{l}+1)] $ including the bias unit  \n$ \\Theta^{(l)} $ : weights for layer $ l $, has dimension $ [ (s_{l}+1) \\times s_{l+1} ] $  \n$ K $ : No of Output Units  \n$ L $ : No of Layers\n\n$ a^{(1)} = X = $ input layer - $ [m \\times (n+1)] $   \n$ \\Theta^{(2)} $ - $ [s_2 \\times (n+1)] $\n\n$ a^{(2)} = g(a^{(1)}. {\\Theta^{(2)}}^T ) $ - $  [m \\times (s_2)] $  ( add ($ a^{(2)}_0 $) )  \n\n\nand so on..."},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Cost Function\n$$ J(\\Theta) = - \\frac{1}{m} [ \\sum^{m}_{i=1} \\sum^{K}_{k = 1} y_{i k} log(h_{\\theta} (x_i)_k ) + (1- y_{i k}) log(1- h_{\\theta} (x_i)_k )] + \\frac{\\lambda}{2m} \\sum^{L}_{l=2} \\sum^{s_l}_{i=1} \\sum^{s_{l+1}}_{j=1} (\\Theta^{(l)}_{ij})^2  $$"},{"cell_type":"markdown","execution_count":1,"metadata":{},"outputs":[],"source":"## Gradient Function\nFor $ L = 4$ $, s_1 = 3$, $s_2 = 5$, $s_3 = 5$, $s_4 = K = 4 $  \n\n$ \\delta^{(l)}_{j} :$ \"error\" in the activation of node $ j$ in layer $l$  \n\n$ \\delta^{(4)} = a^{(4)} - y $ - has dimensions $[m \\times s_4]$  \n\n$ \\delta^{(3)} =  \\delta^{(4)} (\\Theta^{(4)})^T .* g'(z^{(4)}) $ - has dimensions $[m \\times s_4]$  \nwhere, $ g'(z^{(4)}) = g(z^{(4)}) .* (1 - g(z^{(4)})) $  \n\nTherefore,  \n$ \\delta^{(4)} =  \\delta^{(4)} (\\Theta^{(4)})^T .* a^{(4)} .* (1 - a^{(4)} )  $  \n\nand so on..."},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":"import numpy as np\nimport scipy.optimize as op\nimport scipy.io as sio\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":"data = sio.loadmat('Practice\\Machine Learning\\machine-learning-ex3\\ex3\\ex3data1.mat')\nX = data['X']\nX = np.insert(arr=X, obj=0, values=1.0, axis=1)\ny = data['y']\nm, n = X.shape\nK = 10\nlbd = 1\nY = np.zeros((m, 10))\n# saving no of neurons in s\nS = [400, 25, 10]   # excluding bias\n# for predicting digit = 0, we get h high as index 9 (0 based)\n# therefore we create Y as\n# [1, 0, 0, ... 0] for 1\n# [0, 1, 0, ... 0] for 2\n# [0, 0, 0, ... 1] for 0\nfor i in range(1, K+1):\n    Y[np.where(y == i)[0], i-1] = 1"},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[],"source":"def sigmoid(Z):\n    return 1/(1+ 1/(np.e**Z))"},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":"def predict(thetas, X, S):\n    m, n = X.shape\n    \n    theta1 = thetas[:(S[0]+1)*S[1]]\n    theta2 = thetas[(S[0]+1)*S[1]:]\n    theta1 = theta1.reshape((S[1], S[0]+1))    # 25 x 401\n    theta2 = theta2.reshape((S[2], S[1]+1))    # 10 x 26\n\n    theta1 = theta1.T       # n x 25\n    theta2 = theta2.T       # 26 x 10\n    Z2 = X.dot(theta1)      # M x 25\n    A2 = sigmoid(Z2)        \n    A2 = np.insert(arr=A2, obj=0, values=1.0, axis=1)   # M x 26\n    # print(A2.shape)\n    Z3 = A2.dot(theta2)\n    A3 = sigmoid(Z3)        # M X 10\n    # print(A3.shape)\n\n    # choose column wise max index for each example\n    # np.amax(A3, 1) gives each rows max values in an array\n    # np.amax(A3, 1) gives each columns max values in an array\n\n    # np.amax().reshape() make that a column vector\n    # np.where(A3 = ... ) gives two arrays:\n    # 1. Indices of rows for each match\n    # 2. Indices of columns for each match\n    # if an element doesn't match, it is ignored\n    # we only want the indices of columns\n    return np.where(A3 == np.amax(A3, axis=1).reshape((m, 1)) )[1].reshape(m, 1)+1\n"},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[],"source":"# np.where( H == np.amax(H, axis=1).reshape((m, 1)) )[1]\n# thetas = 3d matrix where thetas[0] = theta matrix for layer 1\n# y = m x 10 shaped matrix\ndef costFunc(thetas, X, Y, lbd, S):\n    m, n = X.shape\n    Y = Y.reshape((m, S[2]))\n\n    # print(thetas.shape)\n    theta1 = thetas[:(S[0]+1)*S[1]]\n    theta2 = thetas[(S[0]+1)*S[1]:]\n    theta1 = theta1.reshape((S[1], S[0]+1))    # 25 x 401\n    theta2 = theta2.reshape((S[2], S[1]+1))    # 10 x 26\n\n    theta1 = theta1.T       # n x 25\n    theta2 = theta2.T       # 26 x 10\n    Z2 = X.dot(theta1)      # M x 25\n    A2 = sigmoid(Z2)        \n    A2 = np.insert(arr=A2, obj=0, values=1.0, axis=1)   # M x 26\n    Z3 = A2.dot(theta2)\n    A3 = sigmoid(Z3)        # M X 10\n\n    # cost = -1/m*np.sum(Y*np.log(A3)+(1-Y)*np.log(1-A3))\n    cost = 1/(2*m)*np.sum((A3-Y)**2)\n    cost += lbd /(2*m) * (np.sum(theta1[1:, :]**2) + np.sum(theta2[1:, :]**2) )\n    # print(cost)\n\n    return cost"},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[{"data":{"text/plain":"97.52"},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":"test_result = predict(test_thetas, X, S)\ny[test_result==y].size/m*100"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Backpropagation"},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[],"source":"def sigmoidGrad(Z):\n    p = sigmoid(Z)\n    return p*(1-p)"},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[],"source":"def gradFunc(thetas, X, Y, lbd, S):\n    m, n = X.shape\n    Y = Y.reshape((m, S[2]))\n    \n    theta1 = thetas[:(S[0]+1)*S[1]]\n    theta2 = thetas[(S[0]+1)*S[1]:]\n    theta1 = theta1.reshape((S[1], S[0]+1))    # 25 x 401\n    theta2 = theta2.reshape((S[2], S[1]+1))    # 10 x 26\n\n    Z2 = X.dot(theta1.T)      # M x 25\n    A2 = sigmoid(Z2)\n    A2 = np.insert(arr=A2, obj=0, values=1.0, axis=1)   # M x 26\n    Z3 = A2.dot(theta2.T)\n    A3 = sigmoid(Z3)        # M X 10\n\n    error3 = A3-Y         # M x 10\n    temp3 = error3 * sigmoidGrad(Z3)        # M x 10\n    grad3 = 1/m* error3.T.dot(A2)             # 10 * 26\n    # grad3 = 1/m*temp3.T.dot(A2)             # 10 * 26\n    grad3[:, 1:] += (lbd/m)*theta2[:, 1:]\n\n    error2 = error3.dot(theta2[:, 1:])       # M x 25\n    # error2 = temp3.dot(theta2[:, 1:])       # M x 25\n    temp2 = error2 * sigmoidGrad(Z2)\n    grad2 = 1/m*temp2.T.dot(X)              # 25 x 401\n    grad2[:, 1:] += (lbd/m)*theta1[:, 1:]\n\n    return np.array(np.append(grad2, grad3) ).reshape((10285, 1))"},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":"def randInitWeights(S):\n    import numpy.random as random\n    t1 = random.rand(S[1], S[0]+1)\n    t2 = random.rand(S[2], S[1]+1)\n    return np.array(np.append(t1, t2)).reshape((10285, 1))"},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.12660688219264263 [[ 6.18712766e-05]\n [-2.11248326e-12]\n [ 4.38829369e-13]\n ...\n [ 4.70513145e-05]\n [-5.01718610e-04]\n [ 5.07825789e-04]]\n"}],"source":"test_thetas = sio.loadmat('Practice\\Machine Learning\\machine-learning-ex4\\ex4\\ex4weights.mat')\ntest_thetas = np.array(np.append(test_thetas['Theta1'], test_thetas['Theta2']) )\ntest_thetas=test_thetas.reshape((10285, 1))\ncost = costFunc(test_thetas, X, Y, lbd, S)\ngrad = gradFunc(test_thetas, X, Y, lbd, S)\nprint(cost, grad)"},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":"thetas = randInitWeights(S)"},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[{"data":{"text/plain":"     fun: 0.15680065586765324\n     jac: array([[ 8.25902179e-04],\n       [ 1.93413922e-05],\n       [ 5.86464450e-06],\n       ...,\n       [-5.17236277e-03],\n       [-5.98055707e-03],\n       [-3.14824927e-04]])\n message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n    nfev: 375\n     nit: 25\n  status: 1\n success: True\n       x: array([-0.20853658,  0.09670696,  0.02932322, ...,  0.58665204,\n        0.68342405, -2.06606607])"},"execution_count":183,"metadata":{},"output_type":"execute_result"}],"source":"opt = {'maxiter':500}\ntemp = op.minimize(fun=costFunc, x0=thetas,\n                jac=gradFunc, args=(X, Y, 1, S), \n                method='TNC', options=opt )\ntemp"},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[-2.25623899e-02]\n [-1.05624163e-08]\n [ 2.19414684e-09]\n ...\n [-2.47795788e-01]\n [ 1.28009118e+00]\n [-1.32752042e+00]]\n"},{"data":{"text/plain":"94.12"},"execution_count":184,"metadata":{},"output_type":"execute_result"}],"source":"print(test_thetas)\ntest_result = predict(temp.x, X, S)\ny[test_result==y].size/m*100"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}