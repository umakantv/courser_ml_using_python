{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Neural Network - Backpropagation\n\n## Terminology\n\n$ s_l $ : no of nodes in layer $ l $  \n$ a^{(l)} $ : activation nodes for layer $ l $ , has dimension $ [m \\times (s_{l}+1)] $ including the bias unit  \n$ \\Theta^{(l)} $ : weights for layer $ l $, has dimension $ [ (s_{l}+1) \\times s_{l+1} ] $  \n$ K $ : No of Output Units  \n$ L $ : No of Layers\n\n$ a^{(1)} = X = $ input layer - $ [m \\times (n+1)] $   \n$ \\Theta^{(2)} $ - $ [s_2 \\times (n+1)] $\n\n$ a^{(2)} = g(a^{(1)}. {\\Theta^{(2)}}^T ) $ - $  [m \\times (s_2)] $  ( add ($ a^{(2)}_0 $) )  \n\n\nand so on..."},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Cost Function\n$$ J(\\Theta) = - \\frac{1}{m} [ \\sum^{m}_{i=1} \\sum^{K}_{k = 1} y_{i k} log(h_{\\theta} (x_i)_k ) + (1- y_{i k}) log(1- h_{\\theta} (x_i)_k )] + \\frac{\\lambda}{2m} \\sum^{L}_{l=2} \\sum^{s_l}_{i=1} \\sum^{s_{l+1}}_{j=1} (\\Theta^{(l)}_{ij})^2  $$"},{"cell_type":"markdown","execution_count":1,"metadata":{},"outputs":[],"source":"## Gradient Function\nFor $ L = 4$ $, s_1 = 3$, $s_2 = 5$, $s_3 = 5$, $s_4 = K = 4 $  \n\n$ \\delta^{(l)}_{j} :$ \"error\" in the activation of node $ j$ in layer $l$  \n\n$ \\delta^{(4)} = a^{(4)} - y $ - has dimensions $[m \\times s_4]$  \n\n$ \\delta^{(3)} =  \\delta^{(4)} (\\Theta^{(4)})^T .* g'(z^{(4)}) $ - has dimensions $[m \\times s_4]$  \nwhere, $ g'(z^{(4)}) = g(z^{(4)}) .* (1 - g(z^{(4)})) $  \n\nTherefore,  \n$ \\delta^{(4)} =  \\delta^{(4)} (\\Theta^{(4)})^T .* a^{(4)} .* (1 - a^{(4)} )  $  \n\nand so on..."},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import numpy as np\nimport scipy.optimize as op\nimport scipy.io as sio\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"data = sio.loadmat('Practice\\Machine Learning\\machine-learning-ex4\\ex4\\ex4data1.mat')\nX = data['X']\nX = np.insert(arr=X, obj=0, values=1.0, axis=1)\ny = data['y']\nm, n = X.shape\nK = 10\nlbd = 1\nY = np.zeros((m, 10))\n# saving no of neurons in s\nS = [400, 25, 10]   # excluding bias\n# for predicting digit = 0, we get h high as index 9 (0 based)\n# therefore we create Y as\n# [1, 0, 0, ... 0] for 1\n# [0, 1, 0, ... 0] for 2\n# [0, 0, 0, ... 1] for 0\nfor i in range(1, K+1):\n    Y[np.where(y == i)[0], i-1] = 1"},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":"def sigmoid(Z):\n    return 1/(1+ 1/(np.e**Z))"},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":"def predict(thetas, X, S):\n    m, n = X.shape\n    \n    theta1 = thetas[:(S[0]+1)*S[1]]\n    theta2 = thetas[(S[0]+1)*S[1]:]\n    theta1 = theta1.reshape((S[1], S[0]+1))    # 25 x 401\n    theta2 = theta2.reshape((S[2], S[1]+1))    # 10 x 26\n\n    theta1 = theta1.T       # n x 25\n    theta2 = theta2.T       # 26 x 10\n    Z2 = X.dot(theta1)      # M x 25\n    A2 = sigmoid(Z2)        \n    A2 = np.insert(arr=A2, obj=0, values=1.0, axis=1)   # M x 26\n    # print(A2.shape)\n    Z3 = A2.dot(theta2)\n    A3 = sigmoid(Z3)        # M X 10\n    # print(A3.shape)\n\n    # choose column wise max index for each example\n    # np.amax(A3, 1) gives each rows max values in an array\n    # np.amax(A3, 1) gives each columns max values in an array\n\n    # np.amax().reshape() make that a column vector\n    # np.where(A3 = ... ) gives two arrays:\n    # 1. Indices of rows for each match\n    # 2. Indices of columns for each match\n    # if an element doesn't match, it is ignored\n    # we only want the indices of columns\n    return np.where(A3 == np.amax(A3, axis=1).reshape((m, 1)) )[1].reshape(m, 1)+1\n"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":"# np.where( H == np.amax(H, axis=1).reshape((m, 1)) )[1]\n# thetas = 3d matrix where thetas[0] = theta matrix for layer 1\n# y = m x 10 shaped matrix\ndef costFunc(thetas, X, Y, lbd, S):\n    m, n = X.shape\n    Y = Y.reshape((m, S[2]))\n\n    # print(thetas.shape)\n    theta1 = thetas[:(S[0]+1)*S[1]]\n    theta2 = thetas[(S[0]+1)*S[1]:]\n    theta1 = theta1.reshape((S[1], S[0]+1))    # 25 x 401\n    theta2 = theta2.reshape((S[2], S[1]+1))    # 10 x 26\n\n    theta1 = theta1.T       # n x 25\n    theta2 = theta2.T       # 26 x 10\n    Z2 = X.dot(theta1)      # M x 25\n    A2 = sigmoid(Z2)        \n    A2 = np.insert(arr=A2, obj=0, values=1.0, axis=1)   # M x 26\n    Z3 = A2.dot(theta2)\n    A3 = sigmoid(Z3)        # M X 10\n\n    # cost = -1/m*np.sum(Y*np.log(A3)+(1-Y)*np.log(1-A3))\n    cost = 1/(2*m)*np.sum((A3-Y)**2)\n    cost += lbd /(2*m) * (np.sum(theta1[1:, :]**2) + np.sum(theta2[1:, :]**2) )\n    # print(cost)\n\n    return cost"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Backpropagation"},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":"def sigmoidGrad(Z):\n    p = sigmoid(Z)\n    return p*(1-p)"},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":"def gradFunc(thetas, X, Y, lbd, S):\n    m, n = X.shape\n    Y = Y.reshape((m, S[2]))\n    \n    theta1 = thetas[:(S[0]+1)*S[1]]\n    theta2 = thetas[(S[0]+1)*S[1]:]\n    theta1 = theta1.reshape((S[1], S[0]+1))    # 25 x 401\n    theta2 = theta2.reshape((S[2], S[1]+1))    # 10 x 26\n\n    Z2 = X.dot(theta1.T)      # M x 25\n    A2 = sigmoid(Z2)\n    A2 = np.insert(arr=A2, obj=0, values=1.0, axis=1)   # M x 26\n    Z3 = A2.dot(theta2.T)\n    A3 = sigmoid(Z3)        # M X 10\n\n    error3 = A3-Y         # M x 10\n    temp3 = error3 * sigmoidGrad(Z3)        # M x 10\n    grad3 = 1/m* error3.T.dot(A2)             # 10 * 26\n    # grad3 = 1/m*temp3.T.dot(A2)             # 10 * 26\n    grad3[:, 1:] += (lbd/m)*theta2[:, 1:]\n\n    error2 = error3.dot(theta2[:, 1:])       # M x 25\n    # error2 = temp3.dot(theta2[:, 1:])       # M x 25\n    temp2 = error2 * sigmoidGrad(Z2)\n    grad2 = 1/m*temp2.T.dot(X)              # 25 x 401\n    grad2[:, 1:] += (lbd/m)*theta1[:, 1:]\n\n    return np.array(np.append(grad2, grad3) ).reshape((10285, 1))"},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.12660688219264263 97.52\n"}],"source":"test_thetas = sio.loadmat('Practice\\Machine Learning\\machine-learning-ex4\\ex4\\ex4weights.mat')\ntest_thetas = np.array(np.append(test_thetas['Theta1'], test_thetas['Theta2']) )\ntest_thetas=test_thetas.reshape((10285, 1))\ntest_result = predict(test_thetas, X, S)\nacc = test_result[test_result==y].size/m*100\ncost = costFunc(test_thetas, X, Y, lbd, S)\ngrad = gradFunc(test_thetas, X, Y, lbd, S)\nprint(cost, acc)"},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":"def randInitWeights(S):\n    import numpy.random as random\n    t1 = random.rand(S[1], S[0]+1)/10\n    t2 = random.rand(S[2], S[1]+1)/10\n    return np.array(np.append(t1, t2)).reshape((10285, 1))"},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":"array([[0.08901158],\n       [0.06413426],\n       [0.07916808],\n       ...,\n       [0.06321095],\n       [0.08509999],\n       [0.04424475]])"},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":"thetas = randInitWeights(S)\nthetas"},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":"     fun: 0.17705575324455297\n     jac: array([[-3.69876369e-03],\n       [ 8.61064216e-06],\n       [ 1.04994491e-05],\n       ...,\n       [-2.81670905e-04],\n       [ 1.80337350e-04],\n       [-6.02042087e-04]])\n message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n    nfev: 201\n     nit: 19\n  status: 1\n success: True\n       x: array([ 0.29506661,  0.04305321,  0.05249725, ...,  0.50122868,\n       -0.67929756,  1.66067546])"},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":"opt = {'maxiter':500}\ntemp = op.minimize(fun=costFunc, x0=thetas,\n                jac=gradFunc, args=(X, Y, 1, S), \n                method='TNC', options=opt )\ntemp"},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[-2.25623899e-02]\n [-1.05624163e-08]\n [ 2.19414684e-09]\n ...\n [-2.47795788e-01]\n [ 1.28009118e+00]\n [-1.32752042e+00]]\n"},{"data":{"text/plain":"91.86"},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":"print(test_thetas)\ntest_result = predict(temp.x, X, S)\ny[test_result==y].size/m*100"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Using sklearn"},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":"data = sio.loadmat('Practice\\Machine Learning\\machine-learning-ex4\\ex4\\ex4data1.mat')\nX = data['X']\nX = np.insert(arr=X, obj=0, values=1.0, axis=1)\ny = data['y']\ny2 = y.reshape((1, y.size))\ny2 = y2[0]"},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nclf = MLPClassifier(hidden_layer_sizes=(25, ), max_iter=200)\nX_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.30, random_state=42)"},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Python\\Python37\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  % self.max_iter, ConvergenceWarning)\n"},{"data":{"text/plain":"MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=(25,), learning_rate='constant',\n              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n              validation_fraction=0.1, verbose=False, warm_start=False)"},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":"clf.fit(X_train, y_train)"},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"92.60000000000001\n"}],"source":"pre = clf.predict(X_test)\nm = y_test.size\nacc = y_test[pre==y_test].size/m*100\nprint(acc)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}