{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Decision Tree"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"training_dataset = [\n    ['Green', 3, 'Apple'],\n    ['Yellow', 3, 'Apple'],\n    ['Red', 1, 'Grape'],\n    ['Red', 1, 'Grape'],\n    ['Yellow', 3, 'Lemon']\n]"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"header = [\"color\", \"diameter\", \"label\"]"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"def unique_vals(rows, col):\n    \"\"\"Find the unique values for a column in a datasat. \"\"\"\n    return set([row[col] for row in rows])"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":"{'Apple', 'Grape', 'Lemon'}"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"# Demo\nunique_vals(training_dataset, 2)"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"def class_counts(rows):\n    \"\"\"Counts the number of each type of example in a dataset\"\"\"\n    counts = {}     # a dictionary of label -> count.\n    for row in rows:\n        label = row[-1]\n        if label not in counts:\n            counts[label] = 0\n        counts[label] += 1\n    return counts\n"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"{'Apple': 2, 'Grape': 2, 'Lemon': 1}"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"# Demo of class_counts\nclass_counts(training_dataset)"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"def is_numeric(val):\n    \"\"\"Tes if a value is numeric\"\"\"\n    return isinstance(val, int) or isinstance(val, float)"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"class Question:\n    \"\"\"A Question is used to partition a dataset.training_dataset\n    \n    This class stores a column number (say '0') for which the 'yes/no' \n    question is asked and the value.\n    The match method compares a feature value in an example to the \n    feature value of the question. \"\"\"\n    def __init__(self, col, val):\n        self.col = col\n        self.val = val\n        \n    def match(self, example):\n        val = example[self.col]\n        if is_numeric(val):\n            # why are we only checking for >=\n            return val >= self.val\n        else:\n            return val == self.val\n\n    def __repr__(self):\n        \"\"\" Helper function to print the question\"\"\"\n        condition = \"==\"\n        if is_numeric(self.val):\n            condition = \">=\"\n        return \"Is %s %s %s?\" % (header[self.col], condition, str(self.val) )"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"True"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"# Demo:\nq = Question(0, 'Green')\nq.match(training_dataset[0])"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"def partition(rows, question):\n    \"\"\" Partition the training set in two parts based on a question\"\"\"\n    true_rows, false_rows = [], []\n    for row in rows:\n        if question.match(row):\n            true_rows.append(row)\n        else:\n            false_rows.append(row)\n    return true_rows, false_rows"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":"[['Green', 3, 'Apple'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon']]"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":"# Demo\ntrue_rows, false_rows = partition(training_dataset, Question(0, 'Red'))\n# This will contain all the 'Red' rows.\nfalse_rows"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"def gini(rows):\n    \"\"\" Calculate the Gini Impurity for a list of \n    Gini impurity is a measure of how often a randomly \n    chosen element from the set would be incorrectly labeled \n    if it was randomly labeled according to the distribution of labels \n    in the subset.\n\n    See:\n    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n    \"\"\"\n    counts = class_counts(rows)\n    n = float(len(rows))\n    sum = 0\n    for key in counts:\n        sum += (counts[key]**2)\n    sum /= (n**2)\n    return (1 - sum)\n"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":"0.0"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":"# Demo:\n# Let's look at some example to understand how Gini Impurity works.\n#\n# First, we'll look at a dataset with no mixing.\nno_mixing = [['Apple'],\n              ['Apple']]\n# this will return 0\ngini(no_mixing)"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":"0.5"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":"# Now, we'll look at dataset with a 50:50 apples:oranges ratio\nsome_mixing = [['Apple'],\n               ['Orange']]\n# this will return 0.5 - meaning, there's a 50% chance of misclassifying\n# a random example we draw from the dataset.\ngini(some_mixing)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":"0.8"},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":"# Now, we'll look at a dataset with many different labels\nlots_of_mixing = [['Apple'],\n                  ['Orange'],\n                  ['Grape'],\n                  ['Grapefruit'],\n                  ['Blueberry']]\n# This will return 0.8\ngini(lots_of_mixing)\n#######"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"def info_gain(left, right, cur_unc):\n    \"\"\" Information gain is defined as the \n        uncertainty of the starting node - the weighted impurity of the two children\n    \"\"\"\n    p = float(len(left)/ (len(left)+len(right)) )\n    return cur_unc - p * gini(left) - (1-p)*gini(right)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":"0.64"},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":"current_uncertainty = gini(training_dataset)\ncurrent_uncertainty"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":"0.14"},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":"# How much information do we gain by partioning on 'Green'?\ntrue_rows, false_rows = partition(training_dataset, Question(0, 'Green'))\ninfo_gain(true_rows, false_rows, current_uncertainty)"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":"0.37333333333333335"},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":"# What about if we partioned on 'Red' instead?\ntrue_rows, false_rows = partition(training_dataset, Question(0,'Red'))\ninfo_gain(true_rows, false_rows, current_uncertainty)"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":"[['Red', 1, 'Grape'], ['Red', 1, 'Grape']]"},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":"# It looks like we learned more using 'Red' (0.37), than 'Green' (0.14).\n# Why? Look at the different splits that result, and see which one\n# looks more 'unmixed' to you.\ntrue_rows, false_rows = partition(training_dataset, Question(0,'Red'))\n\n# Here, the true_rows contain only 'Grapes'.\ntrue_rows"},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":"[['Green', 3, 'Apple'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon']]"},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":"# And the false rows contain two types of fruit. Not too bad.\nfalse_rows"},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":"[['Green', 3, 'Apple']]"},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":"# On the other hand, partitioning by Green doesn't help so much.\ntrue_rows, false_rows = partition(training_dataset, Question(0,'Green'))\n\n# We've isolated one apple in the true rows.\ntrue_rows"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.625\n"},{"data":{"text/plain":"[['Yellow', 3, 'Apple'],\n ['Red', 1, 'Grape'],\n ['Red', 1, 'Grape'],\n ['Yellow', 3, 'Lemon']]"},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":"# But, the false-rows are badly mixed up.\nprint(gini(false_rows))\nfalse_rows\n#######"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":"def find_best_split(rows):\n    \"\"\"Find the best question to ask by iterating over every feature / value\n    and calculating the information gain.\"\"\"\n    best_gain = 0\n    best_question = None\n    current_uncertainty = gini(rows)\n    n_features = len(rows[0])-1\n\n    for col in range(n_features):\n        values = set([ row[col] for row in rows ])\n        for val in values:\n            question = Question(col, val)\n            true_rows, false_rows = partition(rows, question)\n            \n            # Skip this split if it doesn't divide the\n            # dataset.\n            if len(true_rows) == 0 or len(false_rows) == 0:\n                continue\n\n            # calculate the information gain for this split\n            gain = info_gain(true_rows, false_rows, current_uncertainty)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_question = question\n    return best_gain, best_question"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":"Is color == Red?"},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":"#######\n# Demo:\n# Find the best question to ask first for our toy dataset.\nbest_gain, best_question = find_best_split(training_dataset)\nbest_question\n# FYI: is color == Red is just as good. See the note in the code above\n# where I used '>='.\n#######"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":"class Leaf:\n    \"\"\"A Leaf node classifies data.\n\n    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n    it appears in the rows from the training data that reach this leaf.\n    \"\"\"\n\n    def __init__(self, rows):\n        self.predictions = class_counts(rows)"},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":"class Decision_Node:\n    \"\"\"A Decision Node asks a question.\n\n    This holds a reference to the question, and to the two child nodes.\n    \"\"\"\n\n    def __init__(self, question, true_branch, false_branch):\n        self.question = question\n        self.true_branch = true_branch\n        self.false_branch = false_branch\n"},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":"def build_tree(rows):\n    \"\"\"Builds the tree.\n\n    Rules of recursion: 1) Believe that it works. 2) Start by checking\n    for the base case (no further information gain). 3) Prepare for\n    giant stack traces.\n    \"\"\"\n\n    # Try partitioing the dataset on each of the unique attribute,\n    # calculate the information gain,\n    # and return the question that produces the highest gain.\n    gain, question = find_best_split(rows)\n\n    # Base case: no further info gain\n    # Since we can ask no further questions,\n    # we'll return a leaf.\n    if gain == 0:\n        return Leaf(rows)\n\n    # If we reach here, we have found a useful feature / value\n    # to partition on.\n    true_rows, false_rows = partition(rows, question)\n\n    # Recursively build the true branch.\n    true_branch = build_tree(true_rows)\n\n    # Recursively build the false branch.\n    false_branch = build_tree(false_rows)\n\n    # Return a Question node.\n    # This records the best feature / value to ask at this point,\n    # as well as the branches to follow\n    # dependingo on the answer.\n    return Decision_Node(question, true_branch, false_branch)"},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"def print_tree(node, spacing=\"\"):\n    \"\"\"World's most elegant tree printing function.\"\"\"\n\n    # Base case: we've reached a leaf\n    if isinstance(node, Leaf):\n        print (spacing + \"Predict\", node.predictions)\n        return\n\n    # Print the question at this node\n    print (spacing + str(node.question))\n\n    # Call this function recursively on the true branch\n    print (spacing + '--> True:')\n    print_tree(node.true_branch, spacing + \"  \")\n\n    # Call this function recursively on the false branch\n    print (spacing + '--> False:')\n    print_tree(node.false_branch, spacing + \"  \")"},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":"my_tree = build_tree(training_dataset)"},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Is color == Red?\n--> True:\n  Predict {'Grape': 2}\n--> False:\n  Is color == Yellow?\n  --> True:\n    Predict {'Apple': 1, 'Lemon': 1}\n  --> False:\n    Predict {'Apple': 1}\n"}],"source":"print_tree(my_tree)"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":"def classify(row, node):\n    \"\"\"See the 'rules of recursion' above.\"\"\"\n\n    # Base case: we've reached a leaf\n    if isinstance(node, Leaf):\n        return node.predictions\n\n    # Decide whether to follow the true-branch or the false-branch.\n    # Compare the feature / value stored in the node,\n    # to the example we're considering.\n    if node.question.match(row):\n        return classify(row, node.true_branch)\n    else:\n        return classify(row, node.false_branch)"},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":"{'Apple': 1}"},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":"#######\n# Demo:\n# The tree predicts the 1st row of our\n# training data is an apple with confidence 1.\nclassify(training_dataset[0], my_tree)\n#######"},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":"def print_leaf(counts):\n    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n    total = sum(counts.values()) * 1.0\n    probs = {}\n    for lbl in counts.keys():\n        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n    return probs"},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":"{'Apple': '100%'}"},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":"#######\n# Demo:\n# Printing that a bit nicer\nprint_leaf(classify(training_dataset[0], my_tree))\n#######"},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":"# Evaluate\ntesting_data = [\n    ['Green', 3, 'Apple'],\n    ['Yellow', 4, 'Apple'],\n    ['Red', 2, 'Grape'],\n    ['Red', 1, 'Grape'],\n    ['Yellow', 3, 'Lemon'],\n]"},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Actual: Apple. Predicted: {'Apple': '100%'}\nActual: Apple. Predicted: {'Apple': '50%', 'Lemon': '50%'}\nActual: Grape. Predicted: {'Grape': '100%'}\nActual: Grape. Predicted: {'Grape': '100%'}\nActual: Lemon. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n"}],"source":"for row in testing_data:\n    print (\"Actual: %s. Predicted: %s\" %\n           (row[-1], print_leaf(classify(row, my_tree))))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}