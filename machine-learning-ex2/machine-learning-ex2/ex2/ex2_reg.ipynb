{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Binary Classification\n## Regularized Logistic Regression\n\n### Hypothesis\n$$ h_{\\theta}(x_i) = g(z_i) $$\nwhere,\n$$ g(z_i) = \\frac{1}{1 + e^{-z_i}} $$\nand,\n$$ z_i = \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + ... + \\theta_n x_{in} $$\n\n### Cost Function\n$$ J_i(h_{\\theta}(x_i)) = -y_i \\log(h_{\\theta}(x_i)) - (1-y_i) \\log(1 - h_{\\theta}(x_i)) $$\n$$ J(\\theta) = \\sum_{i=1}^m J_i(h_{\\theta}(x_i)) + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n\n### Gradient Function\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j }=\\frac{1}{m}\\sum ^m_{i=1} (h_{\\theta } (x^i_{} )-y^i )x^i_j $$ for j = 0\n\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j }=\\frac{1}{m}\\sum ^m_{i=1} (h_{\\theta } (x^i_{} )-y^i )x^i_j + \\lambda \\theta_j $$ for j > 1"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"data = pd.read_csv(\"Practice\\Machine Learning\\machine-learning-ex2\\machine-learning-ex2\\ex2\\ex2data2.txt\", sep=',', header=None)\ndata.columns = ['Test 1', 'Test 2', 'Accepted']\nX = data.iloc[:, 0:2]\ny = data.iloc[:, 2]\nm, n = X.shape\nX.head()"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"plt.figure()\nplt.plot(X[y==1].iloc[:, 0], X[y==1].iloc[:, 1], 'b.', label='Accepted')\nplt.plot(X[y==0].iloc[:, 0], X[y==0].iloc[:, 1], 'r+', label='Rejected')\nplt.xlabel('Microchip Test 1')\nplt.ylabel('Microchip Test 2')\nplt.title('Microchip Accepted/Rejected Data')\nplt.legend()\nplt.show()"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"def sigmoid(Z):\n    return 1/(1+ 1/(np.e**Z))"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"def costFunc(theta, X, y, lbd):\n    m = y.size\n    y = np.array(y)\n    Z = X.dot(theta)\n    H = sigmoid(Z)\n    # print(H)\n    cost = 1/m*(-1 * y.T.dot(np.log(H)) - (1-y).T.dot(np.log(1-H)) )\n    cost = cost + lbd/(2*m)*( (theta.T.dot(theta))-theta[0]**2 )\n    return cost"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"def gradFunc(theta, X, y, lbd):\n    m = y.size\n    Z = X.dot(theta)\n    H = sigmoid(Z)\n    grad = 1/m *( X.T.dot(H-y) + lbd*theta )\n    grad[0] -= lbd/m * theta[0]\n    return grad\n"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"def mapFeatures(X):\n    m,n = X.shape\n    out = pd.DataFrame()\n    X1, X2 = X.iloc[:, 0], X.iloc[:, 1]\n    degree = 6\n    for i in range(7):\n        for j in range(0, i+1):\n            out.insert(out.shape[1], (i-j, j), X1**(i-j) * X2**j)\n    return out"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"X = mapFeatures(X)\nX.head()"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"# initial_theta = np.array([0, 0, 0])\ninitial_theta = np.zeros(28)\ncost = costFunc(initial_theta ,X, y, lbd=1)\ntheta = gradFunc(initial_theta, X, y, 1)\nprint(cost)\nprint(theta.iloc[0:5])"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"initial_theta = np.ones(28)\ncost = costFunc(initial_theta ,X, y, lbd=10)\ntheta = gradFunc(initial_theta, X, y, 10)\nprint(cost)\nprint(theta.iloc[0:5])\n# 0.3460\n#  0.1614\n#  0.1948\n#  0.2269\n#  0.0922"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"import scipy.optimize as op\ninitial_theta = np.zeros(28)\nres = op.fmin_bfgs(f=costFunc, x0=initial_theta, fprime=gradFunc, args=(X,y, 1), maxiter=100)\nprint(res)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"cost = costFunc(res, X, y, 1)\nprint(cost)"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"# Making a prediction\nX_ = pd.DataFrame([-0.25])\nX_.insert(1, 'x2', 1.5)\nX_ = mapFeatures(X_)\nZ = sigmoid(X_.dot(res))\nprint(Z)"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"Z = sigmoid(X.dot(res))\nZ[Z>=0.5] = 1\nZ[Z<0.5] = 0\nprint('Accuracy', Z[Z == y].count()/y.size * 100)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"# Plotting decision boundary\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}